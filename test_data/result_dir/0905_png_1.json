{"file_path": "test_data/pdf_dir/0905_png_1.pdf", "file_name": "0905_png_1.pdf", "fitz_extract_text": null, "post_processed_text": null, "need_ocr": true, "ocr_result": "Large Language Models as a GuideYuwei Zhang Zihan WangJingbo Shang University of California, San Diego (yuz163, ziw224,jshang)@ucsd.edu AhstraciWe introduceCLUSTERLLM, a novel textclus- teringframeworkthatleveragesfeedbackfrom an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervisedmethodsthatbuildsuponsmall' embedders.CLUSTERLLMexhibitstwoin riguingadvantages:(1)itenjoystheemergen capabilityofLLM even if its embeddings are inaccessible; and (2) it understands the user's oreference onclusteringthroughtextual in struction and/or a few annotated data. First, wepromptChatGPTfor insightsonclustering perspectiveby constructing hard triplet ques ions<doesAbettercorrespondtoBthanCy where A,B and C aresimilar data points that be ongtodifferentclustersaccordingtosmallem bedder.We empirically show that this strategy sbotheffectiveforfine-tuningsmallembed der and cost-efficient to queryChatGPT.Sec ond,wepromptChatGPTforhelpsoncluster ing granularityby carefully designed pairwise questions <doAand Bbelong to the same cat- egory>,and tune thegranularityfromcluster hierarchies that is the most consistent with the ChatGPT answers.Extensive experiments on 14datasets showthatCLUSTERLLMconsis- tently improves clustering quality, at an average costof~$o.6'perdataset.  A 8 SOC.AL Introduction1IntroductionTextclusteringhasbeenstudiedforyearsandithas recentlygainednewsignificanceinidentifyingpub licperceptionfromsocialmedia(Parketal.,2022), analysing cause of accidents (Xu et al., 2022) or detecting emerging research topics (Martinez et al., 2022). A common practice in text clustering is toapplyclusteringalgorithms (MacQueen,1967 Zhang et al., 2021a) on top of pre-trained embed- ders (Muennighoff et al., 2022; Wang et al., 2022; Su et al., 2022) which could achieve higher perfor- mance with better pre-training quality. However, 'The cost is calculated with gpt-3.5-turbo +Traditional NotApplicable +ClusterLLM Traditional NotApplicable +ClusterLLM Figure l:Traditional text clustering often employs clus teringalgorithmsontopofthepre-trainedembedders which could produce sub-optimal clustering. LLMs like ChatGPT are not applicable for text clustering directly because of the inaccessible embeddings. CLUSTER- LLM resolves the dilemma by leveraging LLM as a guide ontextclustering. recent instruction-tunedLLMs suchas ChatGPT that demonstrated extraordinary language capabil ities for various natural language applications by following textual instructions, can only be utilized through theAPIs without accessible embedding vectors for clustering. Hence, LLMs can not be directly applied on text clustering tasks. In this paper, we wish to provide insights on the followingquestion: CanweleverageAPI-basedLLMstoguidetext clusteringefficiently? Toapproachsuchachallenge,wefirstdrawinspira- tionfromanobservationthathumansrepresentan instancethroughcomparingwithothers (Nosofsky) 2011). For instance, people often classify a new piece ofmusic into a specificgenreby relating to familiar ones. In fact, pairwise relationships have beenutilizedin spectral clustering(Donathand Hoffman,1972;Cheeger,1970)before.Nonethe- less,naivelytraversingallthepairswithindataset is obviously intractable for querying ChatGPT. In this paper, we propose CLUSTERLLM, a generic framework that utilizes LLM to guide a small embedder for finding text clusters with a low ", "need_gpt": true, "gpt_correct_result": "Large Language Models as a GuideYuwei Zhang Zihan WangJingbo Shang University of California, San Diego (yuz163, ziw224,jshang)@ucsd.edu AhstraciWe introduce CLUSTERLLM, a novel text clustering framework thatleverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that build up on small embedders, CLUSTERLLM exhibits two interesting advantages: (1) it enjoys the emergence capability of LLM even if its embeddings are inaccessible; and (2) it understands the user's preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by construct- ing hard triplet questions <does Abetter correspond to B than C where A, B and C are similar data points that may have been assigned to different clusters according to the small embedder. We empirically show that this strategy is effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for help on clustering granularity by carefully designed pairwise questions <do A and B belong to the same category>, and tune the granularity from cluster hierarchies that is most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that CLUSTERLLM consistently improves clustering quality, at an average cost of ~$0.6 per dataset. A8SOC.AL IntroductionText clustering has been studied for years and has recently gained new significance in identifying public perception from social media (Park et al., 2022), analyzing cause of accidents (Xu et al., 2022) or detecting emerging research topics (Martinez et al., 2022). A common practice in text clustering is to apply clustering algorithms (MacQueen, 1967; Zhang et al., 2021a) on top of pre-trained embedders (Muennighoff et al., 2022; Wang et al., 2022; Su et al., 2022) which could achieve higher performance with better pre-training quality. However, the cost is calculated with gpt-3.5-turbo +Traditional NotApplicable +ClusterLLM Traditional NotApplicable +ClusterLLM Figure l: Traditional text clustering often employ clustering algorithms on top of the pre-trained embedders which could produce sub-optimal clustering. LLMs like ChatGPT are not applicable for text clustering directly because of the inaccessible embeddings. CLUSTER- LLM resolves the dilemma by leveraging LLM as a guide on text clustering. recent instruction-tuned LLMs such as ChatGPT that demonstrated extraordinary language capabilities for various natural language applications by following textual instructions, can only be utilized through the APIs without accessible embedding vectors for clustering. Hence, LLMs can not be directly applied on text clustering tasks. In this paper, we wish to provide insights on the following question: Can we leverage API-based LLMs to guide text clustering efficiently? To approach such a challenge, we first draw inspiration from an observation that humans represent an instance through comparison with others (Nosofsky, 2011). For instance, people often classify a new piece of music into a specific genre by relating to familiar ones. In fact, pairwise relationships have been utilized in spectral clustering (Donath and Hoffman, 1972; Cheeger, 1970) before. Nevertheless, naively traversing all the pairs with information in dataset is obviously intractable for querying ChatGPT. In this paper, we propose CLUSTERLLM, a generic framework that utilizes LLM to guide a small embedder for finding text clusters with a low ã€‚"}