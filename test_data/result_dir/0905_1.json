{"file_path": "test_data/pdf_dir/0905_1.pdf", "file_name": "0905_1.pdf", "fitz_extract_text": "Augmenting Black-box LLMs with Medical Textbooks for Clinical Question\nAnswering\nAnonymous submission\nAbstract\nLarge-scale language models (LLMs), such as ChatGPT,\nare capable of generating human-like responses for various\ndownstream tasks, such as task-oriented dialogues and ques-\ntion answering. However, applying LLMs to medical do-\nmains remains challenging due to their inability to lever-\nage domain-specific knowledge. In this study, we present\nthe Large-scale Language Models Augmented with Med-\nical Textbooks (LLM-AMT), which integrates authorita-\ntive medical textbooks as the cornerstone of its design, en-\nhancing its proficiency in the specialized domain through\nplug-and-play modules, comprised of a Hybrid Textbook Re-\ntriever, supplemented by the Query Augmenter and the LLM\nReader. Experimental evaluation on three open-domain med-\nical question-answering tasks reveals a substantial enhance-\nment in both the professionalism and accuracy of the LLM\nresponses when utilizing LLM-AMT, exhibiting an improve-\nment ranging from 11.4% to 13.2%. Despite being 100×\nsmaller, we found that medical textbooks as the retrieval cor-\npus serves as a more valuable external knowledge source than\nWikipedia in the medical domain. Our experiments show that\ntextbook augmentation results in a performance improvement\nranging from 9.7% to 12.2% over Wikipedia augmentation.\nWe will release our code based on acceptance.\n1\nIntroduction\nLanguage forms the foundation of health and medicine, fa-\ncilitating interactions between individuals and healthcare\nproviders. Recent advancements in Large Language Models\n(LLMs) have opened up new possibilities for exploring the\npotential of artificial intelligence (AI) systems in the medi-\ncal domain, enabling them to comprehend and communicate\nthrough language. This progress holds great promise for en-\nhancing human-AI collaboration, as evidenced by the im-\npressive performance of these models on various medical\nquestion answering datasets (Zhang et al. 2018; Pal, Umap-\nathi, and Sankarasubbu 2022; Jin et al. 2019).\nThe existing LLMs are mainly trained to encode all\nthe world knowledge implicitly into the parameter space.\nHowever, the knowledge encoding process in LLMs may\nlead to information loss and “memory distortion” (Peng\net al. 2023), causing these models to potentially generate\nplausible-sounding but incorrect content (hallucinate). This\ncharacteristic can pose significant challenges, particularly\nwhen such models are applied to critical tasks. Recently,\nthere is increasing interest in augmenting LLMs with ex-\nternal knowledge to deal with this issue, but most of the\npreviously proposed approaches necessitate fine-tuning the\nparameters of LLMs (e.g., Luo et al.; Gao et al.; Singhal\net al.), which can become prohibitively costly as the LLM\nsize grows exponentially.\nThe retrieve-then-read architecture (Lewis et al. 2020;\nKarpukhin et al. 2020; Izacard et al. 2022) has emerged as\nan efficient alternative to compute-intensive fine-tuning. In\nopen-domain QA, a retriever identifies relevant documents\nfor an input question, and then a reader extracts the answer\nusing these documents as context. Prior works either en-\nhanced the retrieval process (Wu et al. 2021; Izacard et al.\n2021) or jointly fine-tuned the reader model (Lewis et al.\n2020; Izacard et al. 2022). More recent advances utilize a\npowerful LLM as the reader, emphasizing LLM-oriented\nadaptation (Shi et al. 2023). However, many rely on general\nknowledge bases like Wikipedia or search engines such as\nGoogle and Bing. Such sources, while vast, might lack depth\nin domain-specific areas like medical or financial fields. Tap-\nping into specialized resources, such as authoritative text-\nbooks, could yield deeper insights in complex domains.\nIn this paper, we propose LLM-AMT, an innovative ap-\nproach tailored to the medical domain. Our primary aim is\nto leverage textbooks as a rich external knowledge source,\nthereby enhancing the capabilities of Large Language Mod-\nels (LLMs) in the medical field. As illustrated in Figure 1,\ngiven an input medical question, the Query Augmenter uti-\nlizes LLMs to rewrite and expand the original question, gen-\nerating more informative queries. Provided with the aug-\nmented query, the Textbook Retriever can integrate different\ntypes of retrievers to recall relevant evidence from the plain\ntext of medical textbooks. Finally, the LLM Reader takes the\nquestion and retrieved evidence as context to obtain the re-\nsponses. On downstream QA tasks, we can obtain multiple\nresponses and then combine them with the majority voting\nto derive the final answer.\nTo demonstrate the effectiveness of the proposed method\nin the medical domain, we conduct empirical evaluations of\nLLM-AMT on three datasets: MedQA-USMLE, MedQA-\nMCMLE, and MedMCQA. We use GPT-3.5 as the baseline\nfor our comparisons. Our results reveal that, using LLM-\nAMT, LLM responses witnessed substantial enhancements\nover the GPT-3.5 baseline, with improvements spanning\nFigure 1: Overview of our proposed pipeline. From left to right, we show the Query Augmenter, the Hybrid Textbook Retriever\nand the LLM Reader through an example of medical question. We have omitted some details from the context for space reasons.\n11.0% to 13.2%. Notably, despite its smaller size, medical\ntextbooks outperformed Wikipedia as an external knowl-\nedge source, resulting in a performance increase between\n9.7% and 12.2%. Furthermore, our human evaluation indi-\ncates that our approach effectively reduced hallucinations by\n16% in the open-ended QA task when compared to the same\nbaseline. This study shows the effectiveness of our approach\nto improving model faithfulness.\nIn summary, the contributions of our research can be ar-\nticulated as follows: (1) We introduce LLM-AMT, a spe-\ncialized pipeline tailored specifically for the medical realm.\nBy integrating authoritative medical textbooks as an external\nknowledge source, we considerably bolster the professional-\nism and precision of LLMs within this specialized domain.\n(2) We highlight the intrinsic value of professional domain\ntextbooks in enhancing the expertise of LLMs through rigor-\nous experimentation, thereby shedding light on an intriguing\navenue for future research. (3) Our ablation study delves into\nthe optimal configuration for the medical domain, investigat-\ning the pivotal roles of the external knowledge retriever, the\nquery augmenter, and the majority voting module within the\noverarching pipeline.\n2\nRelated Work\nIn this section, we review the related work in biomedical\nQA, retrieval-augmented QA and text retrieval.\nBiomedical question answering\nBiomedical QA plays a pivotal role in clinical decision sup-\nport (Ely et al. 2005) and the acquisition of biomedical\nknowledge (Jin et al. 2022). With the rise of pre-trained lan-\nguage models (LMs), there’s been a significant uptick in\nperformance and the emergence of new capabilities across\nvarious natural language processing (NLP) tasks (Chowd-\nhery et al. 2022; Chung et al. 2022; Wei et al. 2022b,a).\nNevertheless, these auto-regressive LLMs, when applied in\ndomains like medicine and healthcare that require intensive\nknowledge or reasoning, are prone to generating hallucina-\ntions and erroneous content. Combining external knowledge\nsources with LLMs is a promising approach to counteract\nthese pitfalls (Mialon et al. 2023).\nRetrieval Augmented Generation\nWhen language models tackle tasks that demand supplemen-\ntary knowledge injection, a retriever can be employed as a\nknowledge interface. This is due to the fact that language\nmodels’ training objectives adhere to linguistic rules rather\nthan the goal of constructing a knowledge base. Thus, a re-\ntriever that provides external knowledge from a knowledge\nbase, can support more background knowledge to reduce the\nhallucination from LMs. This process is understood as re-\ntrieval augmented generation (Lewis et al. 2020).\nThe paradigm of retrieval-augmented generation traces its\nroots to the DrQA framework presented by Chen et al.. In\nthis approach, given a specific question, initial evidence is\nsourced from Wikipedia using heuristic retrievers like TF-\nIDF. Subsequently, a neural model is employed to extract\nanswers from this retrieved evidence. In contrast, Dense Pas-\nsage Retrieval (DPR) (Karpukhin et al. 2020) harnessed the\ncapabilities of pre-trained transformers such as BERT to es-\ntablish a sophisticated neural retriever and reader, achieving\nsuperior performance compared to traditional heuristic mod-\nels. Following this, Retrieval Augmented Generation (RAG)\n(Lewis et al. 2020) redefined the methodology by transi-\ntioning from answer extraction to generation, thus facilitat-\ning the creation of free-form text over mere extractions. In\nthis architecture, both the generator and the retriever un-\ndergo joint optimization. Parallel to these advancements,\nthere are works like REALM (Guu et al. 2020) and RETRO\n(Borgeaud et al. 2022) that aim to enhance language models\nwith retrieval during the pre-training phase. Recently, build-\ning on the adept instruction-following capabilities of Large\nLanguage Models (LLMs), researchers have delved into the\nintegration of LLMs within the retrieval-augmented gener-\nation framework. Notable examples include REPLUG (Shi\net al. 2023) and IC-RALM (Ram et al. 2023).\nHowever, most of the past retrieval setting was on gen-\neral knowledge corpora such as Wikipedia or Web data. In\nthis work, to the best of our knowledge, we present the first\nstudy that utilizes a large amount of high-quality medical\ntextbooks as an external knowledge base, integrating multi-\nple retrievers to retrieve medical background knowledge to\nsupport the reasoning by the language model.\nNeural Text Retrieval\nRecent work on Neural Retrieval using bi-encoder architec-\nture has shown significant improvement over traditional re-\ntrieval methods such as BM25/TF-IDF. The query and doc-\numents are encoded separately by pretrained transformers.\nAnd the query–document similarity was then measured by\nthe corresponding embedding distance. Based on the embed-\nding type, the neural retrieval can be classified into, 1) dense\nretriever, which encodes text into low-dimension dense vec-\ntors. Representative works include DPR (Karpukhin et al.\n2020), ANCE (Xiong et al. 2020), CoCondenser (Gao and\nCallan 2021) etc. 2) sparse retriever, which encodes text\ninto high dimension bag-of-words representation. Represen-\ntative works include DeepImpact (Mallia et al. 2021), uni-\nCOIL (Lin and Ma 2021), SPLADE (Formal, Piwowarski,\nand Clinchant 2021), etc. 3) late interaction retriever, which\nkeeps token level representation and query–document sim-\nilarity is aggregated by the similarity of tokens. Represen-\ntative works include ColBERT (Khattab and Zaharia 2020),\nCOIL (Gao, Dai, and Callan 2021). Although the represen-\ntation type varies, the model are usually optimized w.r.t the\nInfoNCE loss:\nL(q, D+, D−\n1 , D−\n2 , · · · , D−\nn ) = − log p(D = D+ | Q = q)\n= − log\nexp(Sim(q, D+))\nexp(Sim(q, D+)) +\nn�\ni=1\nexp(Sim(q, D−\ni ))\n,\nwhere D+ is the positive document to the query q. and D−\nis hard negatives or in-batch negatives. And the similarity\nfunction Sim(q, D) is usually measured by simple dot prod-\nuct of query and document representation.\nIn this work, we apply various types of neural retrieval\nmethods to medical textbook retrieval tasks to study their\neffectiveness in a domain-specific setting beyond the com-\nmonly used corpora.\n3\nLLM-AMT\nIn this paper, we present LLM-AMT, which is a dedicated\nprocess for answering clinical questions. Figure 1 provides\nan overview of the pipeline, comprising three main steps: (1)\nThe Query Augmenter rewrites and expands the input ques-\ntion into a new query. (2) The Textbook Retriever collects\nrelated evidence from the textbooks using the augmented\nquery. (3) The LLM Reader generates the final answer uti-\nlizing the retrieved evidence. This task can be formulated as\nfollows. Given a medical open-domain QA benchmark rep-\nresented by D = {(x, y)i} for i = 0, 1, 2, . . . , N, where\nxi denotes a medical question input to the pipeline and yi\nstands for the expected output (the correct answer). The\nquestion xi is first rewritten and expanded, resulting in a\nnew query qi. Subsequently, the retriever sources a set of\nparagraphs from medical textbooks, which we refer to as the\nevidence e. The reader then processes the combination [e, x]\nto predict the output ˆy.\nQuery Augmenter\nWe introduce a query augmenting module designed to en-\nhance queries for more effective retrieval in the medical do-\nmain. Our query augmenter consists of two main compo-\nnents: query rewriting and query expansion. For these tasks,\nwe specifically utilize an LLM to achieve both rewriting and\nexpansion of the query.\nThe process of rewriting the query text involves trans-\nforming the terms in the original question into medical ter-\nminology. As illustrated in the first part of Figure 1, we\nutilize a human-written prompt line, “Rephrase the follow-\ning question, abstracting the specific symptoms and con-\nditions of the patient into medical terminology”. This ap-\nproach preserves key information from the original ques-\ntion, such as details related to “cardiac catheterization with\nstenting for unstable angina pectoris” and “mottled, retic-\nulated purplish discoloration of the feet” while facilitat-\ning the translation from general description to medical ter-\nminology (e.g., “Leukocyte count 16,400/mm3; Eosinophils\n11%; Creatinine 4.2mg/dL” is converted into “leukocytosis,\neosinophilia, elevated creatinine levels”).\nFurthermore, to expand the query, we leverage the capa-\nbilities of the LLM by instructing it to answer questions us-\ning a chain-of-thought approach. We provide it with the di-\nrective, You are a medical doctor, systematically and rigor-\nously reasoning through the following question step by step,\nand ultimately providing the answers.” This method enables\nus to uncover more potential directions for retrieval. Ex-\namples include queries like acute kidney injury or chronic\nkidney disease” and “elevated white blood cell count and\neosinophil count”, as illustrated in the first part of Figure 1.\nThrough this approach, we also obtain a foundational frame-\nwork for formulating an answer.\nFinally, we generate the augmented query q by concate-\nnating the rewritten query with the expanded query. The\nQuery Augmenter contributes to offering more comprehen-\nsive information for the retrieval stage, thereby supporting a\nmore nuanced and effective retrieval process.\nTextbook Retrieval Corpus\nMedical textbooks, as the epitome of knowledge in the field\nof human medicine, serve as an invaluable external knowl-\nedge source. While knowledge bases like Wikipedia provide\ngeneral information, textbooks offer richer and more spe-\ncialized domain knowledge. In contrast to search engines\nlike Google Search or Bing Search, the information in text-\nbooks is more reliable. Furthermore, textbooks offer clear\nand concise information, making them a reliable source for\ntext-based retrieval. For our study, we eliminated irrelevant\ninformation, such as diagrams and references, to ensure a fo-\ncused, text-centric corpus. Additionally, longer paragraphs\nin the textbook were broken down according to periods to\nobtain the smallest unit for retrieval, making it easier for the\nLLM reader to use them as context for questions. In this pa-\nper, we utilized 51 textbooks from the MedQA dataset (Jin\net al. 2021), which are designated as the official preparation\nmaterials for the medical licensing exams.\nAn overview of the statistics for the document collection\nin both the textbooks and Wikipedia can be seen in Table 1.\nOur textbook corpus is substantially smaller in scale than\nWikipedia. While Wikipedia comprises millions of para-\ngraphs and billions of tokens, the textbook corpus, though\nspecialized, contains fewer than 350,000 paragraphs and just\nover 27 million tokens. This size difference emphasizes the\ntextbooks’ concentrated domain-specific knowledge.\nMetric\nTextbooks\nWikipedia\n# of paragraphs\n347,797\n21,015,324\n# of tokens\n27,458,075\n2,162,169,361\nTable 1: Overall statistics of the document collection in text-\nbooks and Wikipedia. The Wikipedia dump is from the DPR\nwork (Karpukhin et al. 2020), where Wikipedia documents\nare split into 100-words units.\nHybTextR (Hybrid Textbook Retriever)\nWe integrate various types of retrievers in our textbook re-\ntrieval module to optimize performance, which we refer to as\nthe HybTextR. For sparse retrieval, we follow the SPLADE\n(Formal, Piwowarski, and Clinchant 2021) method. The\nquery and document are encoded separately by BERT, and\nthe MLM layer representation (with dimension\n30k) for\neach token is max aggregated as the text representation.\nReLU function was used to truncate the weights in the repre-\nsentation to be non-negative so that it can fit into an inverted\nindex after quantization at search time. The sparsity of this\nrepresentation is effectively managed by a FLOP loss during\nthe training stage. For dense retrieval, we follow the stan-\ndard pipeline proposed in the DPR work, where the query\nand document embeddings are taken from the CLS token’s\ndense representation in the last layer output (with dimension\n768). At search time, k-NN search is conducted to retrieve\nthe top relevant passages for the given query. For late inter-\naction retrieval, we use the ColBERT setting. For each query\ntoken, the token to document similarity is the maximum of\nthe dot products of the query token to all tokens in docu-\nments. The query–document similarity score is aggregated\nby summation over the similarity score of all query tokens.\nA core problem in the task is how to create supervised\ndata for the neural retriever. As there is no human rele-\nvance judgment for the passages, we treat “helpful” passage\nas positive passage. We first identify questions that GPT-\n3.5-Turbo answers incorrectly when provided without any\ncontextual evidence. Then, using BM25, we recall n pas-\nsages, where n = 32, and concatenate each of them with the\noriginal question to serve as its context. Subsequently, GPT-\n3.5-Turbo is prompted to answer this question. Passages re-\nsulting in correct answers are treated as positive samples,\nwhereas those leading to incorrect answers are categorized\nas hard negatives. Additionally, a subset of passages is ran-\ndomly chosen to act as easy negative samples.\nIn the full pipeline of our evidence retrieval stage, we\nutilize a fusion of sparse retrieval and dense retrieval as\nthe first-phase recall model. Specifically, we normalize the\nscores assigned to each document by both the sparse re-\ntriever and dense retriever, sum them together, and then se-\nlect the top k entries. In this paper, k is consistently set to 32.\nSubsequently, we employ a cross-encoder-based re-ranker to\nreorder the recalled passages.\nLLM Reader\nWithin the LLM reader, evidence retrieved from medical\ntextbooks provides the foundational context for the input\nquestion. This evidence is concatenated with the respective\nquestion, subsequently serving as input for the LLM. To en-\nsure the model’s understanding and its alignment with our\nobjectives, the following structured prompt is employed:\n“You are a medical doctor. Referring to the evidence pro-\nvided, please examine the subsequent medical query and ex-\necute the subsequent tasks:\n1. Critically evaluate the question alongside each potential\nanswer, subsequently determining the correct response.\n2. Appraise the likelihood of correctness for every option,\nrating it on a scale from 0 to 10.”\nUpon employing this structured prompt, the LLM pro-\nduces answers utilizing the Chain-of-Thought prompting\nmethod. Each retrieved evidence will be used to generate\na distinct answer for the same question including the confi-\ndence score pi\nk of each option k and the selected option ansi\nbased on i-th evidence. We then combine N answers using\na method based on majority voting, as detailed below:\nAns = arg maxk\nN\n�\ni=1\nI[ansi == k] ∗ pi\nk\nwhere the confidence scores for each option are summed up\nand the option with the highest confidence score is selected.\nIn this formula, I represents an indicator function, which\noutputs a value of 1 if the condition within its brackets is\nsatisfied (true) and 0 otherwise. This ensures that the confi-\ndence score is only considered in the summation if the cor-\nresponding answer matches the current option k.\n4\nExperiments\nDatasets\nWe evaluate LLM-AMT on three public medical open-\ndomain multiple-choice QA datasets as follows:\nMedQA-USMLE and MedQA-MCMLE\n(Jin et al.\n2021) originate from professional medical board exams in\nthe USA and Mainland China, where doctors are evaluated\non their professional knowledge and ability to make clinical\ndecisions. Questions in these exams are varied and generally\nrequire a deep understanding of related medical concepts\nfrom medical textbooks to answer. In addition to the ques-\ntions and corresponding answers, the datasets also provide\nassociated medical textbook materials. For the USMLE, the\nMedQA-USMLE dataset includes text extracted from a to-\ntal of 18 English medical textbooks used by USMLE candi-\ndates. For the MCMLE, the MedQA-MCMLE dataset fea-\ntures materials from 33 simplified Chinese medical text-\nbooks. These are designated as the official textbooks for\npreparing for the medical licensing exam in Mainland China.\nMedMCQA\n(Pal, Umapathi, and Sankarasubbu 2022) en-\ncompasses a broad spectrum of 2,400 healthcare topics and\n21 distinct medical subjects. The diversity of questions con-\ntained within MedMCQA illustrates the challenges that are\nunique to this dataset. As the questions are derived from\nboth real-world scenarios and simulated examinations, they\nare meticulously crafted by human experts in the field. Con-\nsequently, these questions could serve as a comprehensive\nevaluation of a medical practitioner’s professional compe-\ntencies and expertise.\nQuestion #\nMedQA-\nUSMLE\nMedQA-\nMCMLE\nMedMCQA\nTrain\n10,178\n27,400\n182,822\nDev\n1,272\n3,425\n4,183\nTest\n1,273\n3,426\n6,150\nTable\n2:\nNumber\nof\nQuestions\nin\nMedQA-USMLE,\nMedQA-MCMLE, and MedMCQA\nTable 2 shows the detail of train/dev/test splits of the\ndatasets. We evaluate our pipeline and conduct ablation stud-\nies on the test sets of each dataset.\nBaselines\nOur evaluations encompass two primary categories of mod-\nels. The first group consists of the Closed-Book Models,\nwhich are pre-trained or fine-tuned specifically for the med-\nical domain. These models rely on their internal knowl-\nedge and do not access external databases or texts during\nthe question-answering process. Notable models in this cat-\negory include BioBERT, SciBERT, BioLinkBERT, and\nPubmedBERT (Lee et al. 2020; Beltagy, Lo, and Cohan\n2019; Yasunaga, Leskovec, and Liang 2022; Gu et al. 2021).\nThe second group is Wikipedia-Augmented Models. These\nmodels leverage the knowledge embedded in the Wikipedia\ncorpus to assist in the medical question-answering task. Key\nmodels in this category are Variational ODQA (Li´evin et al.\n2023), Codex 5-shot CoT (Li´evin, Hother, and Winther\n2022), and our replicated models using GPT-3.5-Turbo and\nLLaMA-2-13B (Touvron et al. 2023) that retrieve evidence\nfrom Wikipedia to augment the LLM reader.\nImplementation Details\nWe employed OpenAI’s GPT-3.5-Turbo and LLaMA-2-13B\nas our LLM readers in different experiments. GPT-3.5-\nTurbo, accessed via its API1, handled query rewriting dur-\ning the augmentation phase. In the evidence retrieval stage,\nSPLADE acted as our sparse retriever, DPR was the dense\nretriever, and we incorporated a cross-encoder for reranking.\nThe MS-MARCO dataset was our primary training source\nfor our zero-shot model. Specifics related to fine-tuning,\nsuch as batch size, learning rate, and training rounds, can\nbe found in the supplementary material.\nMain Result\nIn Table 3, we compare various state-of-the-art models with\nour proposed pipeline on MedQA and MedMCQA datasets.\nWhen compared with closed-book models, our approach,\nsupplemented with textbook knowledge, enhances the GPT-\n3.5-Turbo’s baseline performance by margins of 11.4% and\n13.2%. Other domain-specific models, like BioBERT, SciB-\nERT, and PubmedBERT, posted accuracy scores of 36.7%,\n39.0%, and 50.3% respectively on the MedQA dataset.\nWhile these scores underline the efficacy of domain-centric\nmodels, our textbook-augmented method still manages to\nsurpass them. We can also observe a similar trend on the\nMedMCQA dataset.\nComparing our model to the GPT-3.5 + Wiki and LLaMA\n+ Wiki setup, while Wikipedia is a rich source, its knowledge\nmay be more generalized and can lack the depth required\nfor specialized fields like medicine. Often, it tends towards\nproviding more layman or popularized insights, making it\npotentially less valuable for rigorous academic or clinical\ninquiries. Furthermore, GPT-3.5-turbo might already con-\ntain Wikipedia information from its pre-training, possibly\nleading to minimal performance gains (1.0% increase for\nMedQA and 1.7% increase for MedMCQA).\nIn contrast, our method leverages detailed insights from\nmedical textbooks, giving us a clear advantage. We achieve\n12.2% increase over GPT-3.5 + Wiki for MedQA and 9.7%\nfor MedMCQA. The performance distinction emphasizes\nthe significance of integrating deep, specialized medical\nknowledge over broad, surface-level information sources.\nComponent Impact Analysis\nIn our research, we metic-\nulously explored the individual impacts of various compo-\nnents in our designed pipeline, as presented in Table 4. We\n1https://platform.openai.com/docs/guides/gpt\nMethod\nRetriever\nMedQA-\nUSMLE\nMed-\nMCQA\nClosed-Book Model\nRandom\n-\n20.0\n25.0\nBioBERT\n-\n36.7\n37.0\nSciBERT\n-\n-\n39.0\nBioLinkBERT\n-\n45.1\n-\nPubmedBERT\n-\n50.3\n41.0\nLLaMA\n-\n31.4\n35.7\nGPT-3.5\n-\n51.3\n53.9\nWikipedia-Augmented Model\nVariational ODQA\nBM25+DPR\n55.0\n62.9\nCodex 5-shot CoT\nBM25\n60.2\n62.7\nLLaMA + Wikipedia\nDPR\n37.6\n39.5\nGPT-3.5 + Wikipedia\nDPR\n52.3\n55.6\nTextbook-Augmented Model\nLLM-AMT (LLaMA)\nHybTextR\n42.2\n43.8\nLLM-AMT (GPT-3.5)\nHybTextR\n64.5\n65.3\nTable 3: Performance of various state-of-the-art models on\nMedQA and MedMCQA datasets\ndemonstrate the insights derived using the example of results\nfrom the MedQA-USMLE dataset. The trends we observed\nare similar in the other two datasets as well.\n1. Textbook Retriever: Introducing a retrieval mechanism\nsignificantly enhanced the performance. This is evident\nwhen we observe the leap in accuracy from the baseline\nGPT-3.5-Turbo score of 51.3% to 58.6% on the MedQA-\nUSMLE dataset upon adding the retriever. It emphasizes\nthe utility of integrating external knowledge bases to ex-\ntract contextually pertinent information. Moreover, fine-\ntuning the retriever further bolstered the performance,\npushing the accuracy from 62.0% to 64.1% for the “+\nretriever + augmented query” configuration.\n2. Query Augmenter: The efficacy of the query augmenter\nis highlighted when contrasting the results of “+ re-\ntriever” with “+ retriever + augmented query”. Specif-\nically, for the MedQA-USMLE dataset, accuracy in-\ncreased from 58.6% to 62.0% after integrating the aug-\nmented query. This underscores the significance of query\naugmentation in furnishing the retriever with a deeper\ncontext and diverse relevant terminologies, thereby en-\nhancing its capability to procure more pertinent evidence.\n3. LLM Reader: The inclusion of Majority Voting in\nour architecture further boost the performance. For the\nMedQA-USMLE dataset, for example, the score grew\nfrom 62.0% (with augmented queries) to 62.3% af-\nter incorporating majority voting. This shows the LLM\nreader’s ability to consolidate diverse insights from dif-\nferent sources through Majority Voting, which creates a\nmore robust and credible response.\nAblation Study\nHere, we perform ablation studies on both the retrieval\nmechanisms and the majority voting strategy to refine and\nMethod\nMedQA-\nUSMLE\nMedQA-\nMCMLE\nMedMCQA\nGPT-3.5-Turbo\n51.3\n58.2\n53.9\n+ retriever\n58.6\n61.2\n57.1\n+ retriever\n+ majority vote\n60.7\n62.5\n58.8\n+ retriever\n+ augmented query\n62.0\n65.4\n63.1\n+ retriever\n+ augmented query\n+ majority vote\n62.3\n66.8\n63.9\n+ finetuned retriever\n+ augmented query\n64.1\n68.9\n65.2\n+ finetuned retriever\n+ augmented query\n+ majority vote\n64.5\n69.2\n65.3\nTable 4: Performance comparison (% accuracy) of various\napproaches on three medical QA datasets. The table show-\ncases the incremental improvements gained by integrating\ndifferent components. Specifically, the retriever employed is\nHybTextR, and the LLM Reader is GPT-3.5-Turbo.\nidentify the most optimal configuration specifically tailored\nfor question-answering tasks within the medical domain.\nTextbook Retrievers\nIn Table 5, we evaluate the per-\nformance impact of various retrieval mechanisms within\nour pipeline. The late-interaction retriever, particularly the\nColBERT method, demonstrates superior accuracy with\n58.2% on MedQA-USMLE, 62.4% on MedQA-MCMLE,\nand 58.1% on MedMCQA in the zero-shot setting, outper-\nforming both the standalone sparse and dense retrievers.\nWhen combining the semantic capabilities of the dense\nretriever with the precision of the sparse retriever, the system\nachieves better results. The “Sparse + Dense” combination\nyields an accuracy of 60.1% on MedQA-USMLE, 64.9% on\nMedQA-MCMLE, and 58.7% on MedMCQA.\nThe addition of the reranker further optimizes perfor-\nmance. The “Dense + Rerank” configuration registers scores\nof 60.6%, 65.4%, and 61.8% across the three datasets,\nrespectively. Notably, the “HybTextR”, which integrates\nsparse, dense, and reranker components, attains the high-\nest performances of 62.0% for MedQA-USMLE, 68.9% for\nMedQA-MCMLE, and 65.2% for MedMCQA. This consol-\nidated approach demonstrates the benefits of a comprehen-\nsive retrieval strategy in the medical domain.\nMajority Voting\nIn our LLM reader module related to\nmajority voting, we conducted experiments on different\ndatasets with various top-k settings, distinguishing between\ncases with and without fine-tuning, as shown in Figure 2.\nThe results indicate that for the fine-tuned retriever, the op-\ntimal setting is top-k=2, while for the retriever without fine-\ntuning, the best performance is achieved with top-k=4. This\nsuggests that fine-tuning significantly alters the optimal top-\nk configuration for maximizing accuracy.\nZero-shot\nFine-tuned\nMedQA-USMLE MedQA-MCMLE MedMCQA MedQA-USMLE MedQA-MCMLE MedMCQA\nBM25\n55.6\n59.7\n55.2\n–\n–\n–\nSparse\n57.4\n60.4\n57.5\n59.3\n62.9\n59.6\nDense\n59.7\n61.0\n57.7\n60.9\n63.8\n59.3\nColBERT\n58.2\n62.4\n58.1\n61.5\n64.1\n60.4\nSparse + Dense\n60.1\n64.9\n58.7\n62.7\n65.5\n61.9\nSparse + Rerank\n59.5\n63.8\n59.2\n61.3\n65.2\n62.8\nDense + Rerank\n60.6\n65.4\n61.8\n63.7\n65.3\n64.6\nHybTextR\n62.0\n64.4\n63.1\n64.1\n68.9\n65.2\nTable 5: Evaluation of Retrieval and Reranking Strategies on the Performance of LLM-AMT\nFigure 2: Accuracy of LLM-AMT Under Different Top-k\nSelections in Majority Voting. GPT-3.5-Turbo was used as\nthe LLM Reader.\nFurther Discussion\nIn this section, we provide an in-depth discussion and fur-\nther assessment of our models, especially their capabilities\nin handling non-multiple-choice medical QA tasks.\nNon-multiple-choice QA Task\nTo emulate the real-world\napplication of models in medical question answering, we\nrandomly selected a diverse subset of 100 questions from\nthe MedQA-USMLE dataset and generated answers directly\nwithout accessing the options. For evaluation, individuals\nwith a medical background were enlisted to critically assess\nthe quality of each answer. These answers were categorized\ninto four distinct tiers:\nTiers\nGPT-3.5\nLLM-AMT\nCorrect\n27\n36\nMostly Correct\n10\n12\nPartially Correct\n14\n19\nWrong\n49\n33\nTable 6: Evaluation of the Non-multiple-choice Medical\nQuestion Answering Task. GPT-3.5 as the LLM Reader.\n• Correct: Fully accurate and comprehensive.\n• Mostly Correct: Accurate with minor omissions.\n• Partially Correct: Contains accurate elements but\nmisses vital details in the response.\n• Wrong: Largely inaccurate or off-target.\nOur LLM-AMT model surpassed the GPT-3.5-Turbo\nbaseline in the non-multiple-choice QA task, delivering 36\ncorrect answers to the baseline’s 27. Notably, LLM-AMT\nprovided more partially correct answers (19 vs. 14) and\nfewer errors (33 vs. 49). This underscores the model’s en-\nhanced accuracy in the medical QA domain, as detailed in\nTable 6. The superior performance of LLM-AMT in the non-\nmultiple-choice QA task not only illustrates its advanced\ncapabilities but also emphasizes its potential for practical\napplication in real-world medical scenarios. Such advance-\nments can be instrumental in aiding medical professionals\nwith more accurate and reliable information.\n5\nConclusion\nWe introduced LLM-AMT, a novel pipeline optimized for\nmedical tasks, harnessing authoritative medical textbooks\nto enhance LLMs’ accuracy and professionalism. Empiri-\ncal evaluations reinforced the value of integrating domain-\nspecific textbooks with LLMs, providing an avenue for fu-\nture studies. Further, our ablation study delineated the sig-\nnificance of external knowledge retrieval, query augmenta-\ntion, and majority voting mechanisms within our proposed\narchitecture. These findings set a precedent for advancing\nspecialized domain-aware models, especially in the context\nof medical informatics and healthcare AI applications.\nReferences\nBeltagy, I.; Lo, K.; and Cohan, A. 2019. SciBERT: A pre-\ntrained language model for scientific text. arXiv preprint\narXiv:1903.10676.\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\nford, E.; Millican, K.; Van Den Driessche, G. B.; Lespiau,\nJ.-B.; Damoc, B.; Clark, A.; et al. 2022. Improving language\nmodels by retrieving from trillions of tokens. In Interna-\ntional conference on machine learning, 2206–2240. PMLR.\nChen, D.; Fisch, A.; Weston, J.; and Bordes, A. 2017. Read-\ning wikipedia to answer open-domain questions.\narXiv\npreprint arXiv:1704.00051.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.\n2022. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416.\nEly, J. W.; Osheroff, J. A.; Chambliss, M. L.; Ebell, M. H.;\nand Rosenbaum, M. E. 2005. Answering physicians’ clinical\nquestions: obstacles and potential solutions. Journal of the\nAmerican Medical Informatics Association, 12(2): 217–224.\nFormal, T.; Piwowarski, B.; and Clinchant, S. 2021.\nSPLADE: Sparse lexical and expansion model for first stage\nranking. In Proceedings of the 44th International ACM SI-\nGIR Conference on Research and Development in Informa-\ntion Retrieval, 2288–2292.\nGao, J.; Xiong, C.; Bennett, P.; and Craswell, N. 2022. Neu-\nral approaches to conversational information retrieval. arXiv\npreprint arXiv:2201.05176.\nGao, L.; and Callan, J. 2021.\nCondenser: a pre-\ntraining architecture for dense retrieval.\narXiv preprint\narXiv:2104.08253.\nGao, L.; Dai, Z.; and Callan, J. 2021. COIL: Revisit exact\nlexical match in information retrieval with contextualized in-\nverted list. arXiv preprint arXiv:2104.07186.\nGu, Y.; Tinn, R.; Cheng, H.; Lucas, M.; Usuyama, N.; Liu,\nX.; Naumann, T.; Gao, J.; and Poon, H. 2021.\nDomain-\nspecific language model pretraining for biomedical natural\nlanguage processing. ACM Transactions on Computing for\nHealthcare (HEALTH), 3(1): 1–23.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.\n2020. Retrieval augmented language model pre-training. In\nInternational conference on machine learning, 3929–3938.\nPMLR.\nIzacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski,\nP.; Joulin, A.; and Grave, E. 2021. Unsupervised dense in-\nformation retrieval with contrastive learning. arXiv preprint\narXiv:2112.09118.\nIzacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.;\nSchick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave,\nE. 2022. Few-shot learning with retrieval augmented lan-\nguage models. arXiv preprint arXiv:2208.03299.\nJin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and\nSzolovits, P. 2021. What disease does this patient have? a\nlarge-scale open domain question answering dataset from\nmedical exams. Applied Sciences, 11(14): 6421.\nJin, Q.; Dhingra, B.; Liu, Z.; Cohen, W. W.; and Lu, X. 2019.\nPubmedqa: A dataset for biomedical research question an-\nswering. arXiv preprint arXiv:1909.06146.\nJin, Q.; Yuan, Z.; Xiong, G.; Yu, Q.; Ying, H.; Tan, C.; Chen,\nM.; Huang, S.; Liu, X.; and Yu, S. 2022. Biomedical ques-\ntion answering: a survey of approaches and challenges. ACM\nComputing Surveys (CSUR), 55(2): 1–36.\nKarpukhin, V.; O˘guz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov,\nS.; Chen, D.; and Yih, W.-t. 2020.\nDense passage re-\ntrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906.\nKhattab, O.; and Zaharia, M. 2020. Colbert: Efficient and\neffective passage search via contextualized late interaction\nover bert. In Proceedings of the 43rd International ACM SI-\nGIR conference on research and development in Information\nRetrieval, 39–48.\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and\nKang, J. 2020. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinfor-\nmatics, 36(4): 1234–1240.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;\nGoyal, N.; K¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt¨aschel,\nT.; et al. 2020.\nRetrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in Neural Infor-\nmation Processing Systems, 33: 9459–9474.\nLi´evin, V.; Hother, C. E.; and Winther, O. 2022. Can large\nlanguage models reason about medical questions?\narXiv\npreprint arXiv:2207.08143.\nLi´evin, V.; Motzfeldt, A. G.; Jensen, I. R.; and Winther,\nO. 2023.\nVariational Open-Domain Question Answering.\nIn International Conference on Machine Learning, 20950–\n20977. PMLR.\nLin, J.; and Ma, X. 2021. A few brief notes on deepimpact,\ncoil, and a conceptual framework for information retrieval\ntechniques. arXiv preprint arXiv:2106.14807.\nLuo, R.; Sun, L.; Xia, Y.; Qin, T.; Zhang, S.; Poon, H.;\nand Liu, T.-Y. 2022. BioGPT: generative pre-trained trans-\nformer for biomedical text generation and mining. Briefings\nin Bioinformatics, 23(6): bbac409.\nMallia, A.; Khattab, O.; Suel, T.; and Tonellotto, N. 2021.\nLearning passage impacts for inverted indexes. In Proceed-\nings of the 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, 1723–\n1727.\nMialon, G.; Dess`ı, R.; Lomeli, M.; Nalmpantis, C.; Pa-\nsunuru, R.; Raileanu, R.; Rozi`ere, B.; Schick, T.; Dwivedi-\nYu, J.; Celikyilmaz, A.; et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nPal, A.; Umapathi, L. K.; and Sankarasubbu, M. 2022.\nMedmcqa: A large-scale multi-subject multi-choice dataset\nfor medical domain question answering. In Conference on\nHealth, Inference, and Learning, 248–260. PMLR.\nPeng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y.; Hu, Y.;\nHuang, Q.; Liden, L.; Yu, Z.; Chen, W.; et al. 2023. Check\nyour facts and try again: Improving large language models\nwith external knowledge and automated feedback.\narXiv\npreprint arXiv:2302.12813.\nRam, O.; Levine, Y.; Dalmedigos, I.; Muhlgay, D.; Shashua,\nA.; Leyton-Brown, K.; and Shoham, Y. 2023.\nIn-context\nretrieval-augmented language models.\narXiv preprint\narXiv:2302.00083.\nShi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.; Lewis,\nM.; Zettlemoyer, L.; and Yih, W.-t. 2023. Replug: Retrieval-\naugmented black-box language models.\narXiv preprint\narXiv:2301.12652.\nSinghal, K.; Tu, T.; Gottweis, J.; Sayres, R.; Wulczyn,\nE.; Hou, L.; Clark, K.; Pfohl, S.; Cole-Lewis, H.; Neal,\nD.; et al. 2023.\nTowards expert-level medical question\nanswering with large language models.\narXiv preprint\narXiv:2305.09617.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nWei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler,\nD.; et al. 2022a. Emergent abilities of large language mod-\nels. arXiv preprint arXiv:2206.07682.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V.; Zhou, D.; et al. 2022b.\nChain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nWu, B.; Zhang, Z.; Wang, J.; and Zhao, H. 2021. Sentence-\naware contrastive learning for open-domain passage re-\ntrieval. arXiv preprint arXiv:2110.07524.\nXiong, L.; Xiong, C.; Li, Y.; Tang, K.-F.; Liu, J.; Bennett,\nP.; Ahmed, J.; and Overwijk, A. 2020. Approximate near-\nest neighbor negative contrastive learning for dense text re-\ntrieval. arXiv preprint arXiv:2007.00808.\nYasunaga, M.; Leskovec, J.; and Liang, P. 2022. Linkbert:\nPretraining language models with document links.\narXiv\npreprint arXiv:2203.15827.\nZhang, X.; Wu, J.; He, Z.; Liu, X.; and Su, Y. 2018. Medical\nexam question answering with large-scale reading compre-\nhension. In Proceedings of the AAAI conference on artificial\nintelligence, volume 32.\n", "post_processed_text": null, "need_ocr": true, "ocr_result": "ugmenting Black-box LLMs with Medical Textbooks for Clinical QuestioAnonymous submission AhstractAhstract Large-scale language models (LLMs), such as ChatGPT) are capable of generating human-like responses for various downstream tasks, such as task-oriented dialogues and ques- tion answering. However, applying LLMs to medical do mains remains challenging due to their inability to lever- age domain-specific knowledge. In this study, we present the Large-scale Language Models Augmented with Med- ical Textbooks (LLM-AMT), which integrates authorita- tive medical textbooks as the cornerstone of its design, en- hancing its proficiency in the specialized domain through plug-and-play modules, comprised of a Hybrid Textbook Re- triever, supplemented by the Query Augmenter and the LLM Reader.Experimental evaluation on three open-domain med ical question-answering tasks reveals a substantial enhance ment in both the professionalism and accuracy of the LLM responses when utilizing LLM-AMT, exhibiting an improve- ment ranging from 11.4% to 13.2%.Despite being 100x smaller, we found that medical textbooks as the retrieval cor pus serves as a more valuable external knowledge source than Wikipedia in the medical domain. Our experiments show that textbook augmentation results in a performance improvement ranging from 9.7% to 12.2% over Wikipedia augmentation. We will release our code based on acceptance IntroductionLanguageformsthefoundationofhealthandmedicine,fa- cilitating interactions between individuals and healthcare oroviders.RecentadvancementsinLargeLanguageModels (LLMs) have opened up new possibilities for exploring the potential of artificial intelligence (AI) systems in the medi- caldomain,enablingthemtocomprehendandcommunicate hrough language. This progress holds great promise for en- hancinghuman-AIcollaboration,asevidencedbytheim- oressiveperformanceofthesemodelsonvariousmedical question answering datasets (Zhang et al. 2018; Pal, Umap- athi, and Sankarasubbu 2022; Jin et al.2019). The existing LLMs are mainly trained to encode all he world knowledge implicitly into the parameter space. However, the knowledge encoding process in LLMs may lead to information loss and “memory distortion\" (Peng et al. 2023), causing these models to potentially generate plausible-sounding but incorrect content (hallucinate). This characteristic can pose significant challenges, particularly when such models are applied to critical tasks. Recently. ternal knowledge to deal with this issue, but most of the previously proposed approaches necessitate fine-tuning the parameters of LLMs (e.g., Luo et al.; Gao et al.; Singhal et al.), which can become prohibitively costly as the LLM sizegrowsexponentially The retrieve-then-read architecture (Lewis et al. 2020: Karpukhin et al. 2020; Izacard et al. 2022) has emerged as an efficient alternative to compute-intensive fine-tuning. In open-domain QA, a retriever identifies relevant documents foraninputquestion,andthenareaderextractstheanswer usingthesedocumentsascontext.Priorworkseitheren- hanced the retrieval process (Wu et al. 2021; Izacard et al.) 2021) or jointly fine-tuned the reader model (Lewis et al. 2020; Izacard et al. 2022). More recent advances utilize a powerful LLM as the reader, emphasizing LLM-oriented adaptation (Shi et al. 2023). However, many rely on general knowledgebaseslikeWikipediaorsearchenginessuchas GoogleandBing.Suchsources.whilevast,mightlackdepth in domain-specific areas like medical or financial fields. Tap- pinginto specializedresources,suchas authoritativetext books, could yield deeper insights in complex domains. In this paper, we propose LLM-AMT, an innovative ap- proach tailored to the medical domain. Our primary aim is toleveragetextbooksasarichexternalknowledgesource. therebyenhancingthecapabilitiesofLargeLanguageMod- els (LLMs) in the medical field. As illustrated in Figure 1, givenaninputmedicalquestion,theQueryAugmenteruti- lizesLLMstorewriteandexpandtheoriginalquestion,gen erating more informative queries. Provided with the aug- mented query, the Textbook Retriever can integrate different ypesofretrieverstorecallrelevantevidencefromtheplain textofmedicaltextbooks.Finally,theLLMReadertakesthe question and retrieved evidence as context to obtain the re- sponses.OndownstreamQAtasks,wecanobtainmultiple responses and then combine them with the majority voting to derive the final answer. To demonstrate the effectiveness of the proposed method in the medical domain, we conduct empirical evaluations of LM-AMTonthreedatasets:MedQA-USMLE,MedQA- MCMLE, and MedMCQA. We use GPT-3.5 as the baseline for our comparisons. Our results reveal that, using LLM- AMT, LLM responses witnessed substantial enhancements over the GPT-3.5 baseline, with improvements spanning Query Augmenter福 disease or\"blue toe syndrome\"). Cholesterol emb lization occurs when atherosclerotic plaques in larger arteries break off and embolize to smaller vessels, leading to occlusion and tssue damage. Inthis case, the emboli have affected the kidneys, leading to decreased urinary output and malaise. 11.0%to13.2%.Notably,despiteitssmallersize,medical textbooks outperformed Wikipedia as an external knowl- edge source, resulting in a performance increase between 9.7%and12.2%.Furthermoreourhumanevaluationindi- cates that our approach effectively reduced hallucinations by 16% in the open-ended QA task when compared to the same baseline. This study shows the effectiveness of our approach toimprovingmodelfaithfulness. In summary, the contributions of our research can be ar- ticulated as follows: (1) We introduce LLM-AMT, a spe- cialized pipeline tailored specifically for the medical realm. By integrating authoritative medical textbooks as an external Knowledgesource,weconsiderablybolstertheprofessional- sm and precision of LLMs within this specialized domain. (2)We highlight the intrinsic value of professional domain textbooks in enhancing the expertise of LLMs through rigor- ous experimentation, thereby shedding light on an intriguing avenue for future research. (3) Our ablation study delves into the optimal configuration for the medical domain, investigat- ing the pivotal roles of the external knowledge retriever, the query augmenter, and the majority voting module within the overarchingpipeline 2Related Worknthis section,wereviewtherelatedworkinbiomedi A,retrieval-augmentedQAandtextretrieval Biomedical question answeringBiomedical QA plays a pivotal role in clinical decision sup- port(Elyetal.2005)andtheacquisitionofbiomedical knowledge (Jin et al. 2022). With the rise of pre-trained lan- guage models (LMs), there's been a significant uptick in performanceandtheemergenceofnewcapabilitiesacross various natural language processing (NLP) tasks (Chowd- hery et al. 2022; Chung et al. 2022; Wei et al. 2022b,a). Nevertheless, these auto-regressive LLMs, when applied in domainslikemedicineandhealthcarethatrequireintensive knowledge or reasoning, are prone to generating hallucina- ionsanderroneouscontent.Combiningexternalknowledge sourceswithLLMs is apromisingapproachto counteract these pitfalls (Mialon et al. 2023). RetrievalAugmented Generationsthatdemandsupplemen- taryknowledgeinjection,aretrievercanbeemployedasa knowledge interface. This is due to the fact that language models' training objectives adhere to linguistic rules rather thanthegoalof constructingaknowledgebase.Thus,are- riever that provides external knowledge from a knowledge oase, can supportmore backgroundknowledgeto reduce the nallucination from LMs. This process is understood as re- rieval augmented generation (Lewis et al. 2020). The paradigm of retrieval-augmented generation traces its rootstotheDrQAframeworkpresentedby nenetal..In this approach, given a specific question, initial evidence is sourced from Wikipedia using heuristic retrievers like TF- IDF. Subsequently, a neural model is employed to extract answersfromthisretrievedevidence.Incontrast,DensePas- sage Retrieval (DPR) (Karpukhin et al. 2020) harnessed the capabilitiesofpre-trainedtransformerssuchasBERTtoes- tablishasophisticatedneuralretrieverand reader,achieving superior performance compared to traditional heuristic mod- els.Followingthis,RetrievalAugmentedGeneration(RAG) Lewisetal.2020)redefinedthemethodologybytransi- tioning from answer extraction to generation, thus facilitat- ing the creation of free-form text over mere extractions. In this architecture, both the generator and the retriever un- dergo joint optimization. Parallel to these advancements, hereareWorkslikeREALM(Guuetal.202O)andRETRO Borgeaudetal.2022)thataimtoenhancelanguagemodels with retrieval during the pre-training phase. Recently, build- ing on the adept instruction-following capabilities of Large Language Models (LLMs), researchers have delved into the integration of LLMs within the retrieval-augmented gener- ation framework. Notable examples include REPLUG (Shi etal.2023)andIC-RALM(Rametal.2023) However, most of the past retrieval setting was on gen- eral knowledge corpora such as Wikipedia or Web data. In hiswork,tothebestofourknowledge,wepresentthefirst study that utilizes a large amount of high-quality medical textbooks as an external knowledge base, integrating multi- pleretrieverstoretrievemedicalbackgroundknowledgeto support the reasoning by the language model. Neural Text RetrievalRecentwork onNeuralRetrievalusingbi-encoderarchitec- turehasshownsignificantimprovementovertraditionalre trieval methods such as BM25/TF-IDF. The query and doc- uments are encoded separately by pretrained transformers. Andthequery-documentsimilaritywasthenmeasuredby the corresponding embedding distance. Based on the embed- ding type, the neural retrieval can be classified into, 1) dense retriever,whichencodestextintolow-dimensiondensevec ors.Representativeworks includeDPR(Karpukhinet al. 2020), ANCE (Xiong et al. 2020), CoCondenser (Gao and Callan 2021) etc. 2) sparse retriever, which encodes text ntohighdimensionbag-of-wordsrepresentation.Represen- tative works include Deeplmpact (Mallia et al. 202i), uni- COIL (Lin and Ma 2021), SPLADE (Formal, Piwowarski, andClinchant2021),etc.3)lateinteractionretriever,which keeps token level representation and query-document sim- ilarity is aggregated by the similarity of tokens. Represen ativeworksincludeColBERT(KhattabandZaharia202O) COIL(Gao,Dai,andCallan2021).Althoughtherepresen tation type varies, the model are usually optimized w.r.t the InfoNCEloss: C(q, D+,Di, D2,..: , Dn) = - log p(D = D+ I Q = q) exp(Sim(q, D+)) exp(Sim(q, D+) + ≥ exp(Sim(q, D; )) whereD+ isthepositivedocumenttothequeryq.andD- is hard negatives or in-batch negatives. And the similarity function Sim(q, D) is usually measured by simple dot prod- ictofqueryanddocumentrepresentation. In this work, we apply various types of neural retrieval methods to medical textbook retrieval tasks to study their effectiveness in a domain-specific setting beyond the com- monly used corpora. 3LLM-AMTInthispaper,wepresentI :whichisadedicateo processfor answering clinical questions.Figure1provides anoverviewofthepipeline,comprisingthreemainsteps:(1) The Query Augmenter rewrites and expands the input ques- tion into a new query. (2) The Textbook Retriever collects relatedevidencefromthetextbooksusingtheaugmented query. (3) The LLM Reader generates the final answer uti- izingtheretrieved evidence.Thistaskcanbeformulatedas follows. Given a medical open-domain QA benchmark rep- esented by D = {(a,y)i} for i = 0,1,2,..., N, where Ci denotes a medical question input to the pipeline and yi standsfortheexpectedoutput(thecorrectanswer).The question  is first rewritten and expanded, resulting in a new query qi. Subsequently, the retriever sources a set of paragraphs from medical textbooks, which we refer to as the evidence e. The reader then processes the combination [e, ] to predict the output y Query AugmenterWe introduce a query augmenting module designed to en- hance queries for more effective retrieval in the medical do- main.Ourqueryaugmenterconsistsoftwomaincompo- nents:queryrewriting andqueryexpansion.Forthesetasks. we specifically utilize an LLM to achieve both rewriting and expansionofthequery. Theprocess ofrewritingthequerytextinvolves trans orming the terms in the original question into medical ter- minology.AsillustratedinthefirstpartofFigure1,we atilize a human-written prompt line, “Rephrase the follow- ngquestion,abstractingthespecificsymptomsandcon ditions of the patient into medical terminology\". This ap- proach preserveskey informationfrom the original ques- ion, such as details related to \"cardiac catheterization with tenting for unstable angina pectoris\" and “mottled, retic- lated purplishdiscolorationof thefeet\"whilefacilitat ngthetranslationfromgeneraldescriptiontomedical ter ninology (e.g.,\"Leukocyte count 16,400/mm3; Eosinophils 1%;Creatinine4.2mg/dL\"isconvertedinto\"leukocytosis, eosinophilia, elevated creatinine levels\") Furthermore, to expand the query, we leverage the capa- ilitiesof theLLMbyinstructingittoanswerquestionsus. ng a chain-of-thought approach. We provide it with the di- rective, You are a medical doctor, systematically and rigor ously reasoning through the following question step by step and ultimately providing the answers.\" This method enables ustouncovermorepotentialdirectionsforretrieval.Ex- amplesincludequerieslikeacutekidneyinjuryorchronic kidney disease\" and \"elevated white blood cell count and eosinophil count\",asillustratedinthefirstpartofFigure1 Through this approach, we also obtain a foundational frame- workforformulatingananswer. Finally, we generate the augmented query q by concate- nating the rewritten query with the expanded query. The Query Augmenter contributes to offering more comprehen- sive information for the retrieval stage, thereby supporting a morenuancedandeffectiveretrievalprocess Textbook Retrieval CorpusMedicaltextbooks,astheepitomeofknowledgeinthefield ofhumanmedicine,serveasaninvaluableexternalknowl edge source. While knowledge bases like Wikipedia provide general information, textbooks offer richer and more spe- cialized domain knowledge. In contrast to search engines ikeGoogleSearchorBingSearch,theinformationintext- oooksismorereliable.Furthermore,textbooksofferclear andconciseinformation,makingthemareliablesourcefor ext-based retrieval.For our study,we eliminated irrelevant nformation,such as diagramsandreferences,toensureafo- cused,text-centriccorpus.Additionally,longerparagraphs inthetextbookwerebrokendownaccordingtoperiodsto obtain the smallest unit for retrieval, making it easier for the LLM reader to use them as context for questions. In this pa- per, we utilized 51 textbooks from theMedQAdataset (Jin et al. 2021), which are designated as the official preparation materialsforthemedicallicensingexams. Anoverviewofthe statisticsforthedocumentcollection nboththe textbooks and Wikipedia canbe seenin Table1. Ourtextbookcorpusissubstantiallysmallerinscalethan Wikipedia.WhileWikipediacomprisesmillionsofpara- graphs and billions of tokens, the textbook corpus, though specialized, contains fewer than 350,000 paragraphs and just over 27 million tokens. This size difference emphasizes the textbooks' concentrated domain-specific knowledge. Table l:Overall statistics of the document collection in text- oooks and Wikipedia. The Wikipedia dump is from the DPR work (Karpukhin et al. 2020), where Wikipedia documents are split into 100-words units. HybTextR (Hybrid Textbook Retriever)Weintegrate rieval module to optimize performance, which we refer to as the HybTextR.For sparseretrieval, we follow the SPLADE (Formal, Piwowarski, and Clinchant 2021) method. The gueryanddocumentareencodedseparatelybyBERT,and the MLM layer representation (with dimension 30k)for eachtokenismaxaggregatedasthetextrepresentation. ReLU function was used to truncate the weights in the repre- sentation to be non-negative so that it can fit into an inverted index after quantization at search time. The sparsity of this representation is effectively managed bya FLOPloss during dard pipeline proposed in the DPR work, where the query and document embeddings are taken from the CLS token's Henserepresentationinthelastlayeroutput(withdimension 768).At searchtime,k-NN searchisconductedtoretrieve he top relevant passages for the given query. For late inter- actionretrieval,weusetheColBERTsetting.Foreachquery oken,thetokentodocumentsimilarityisthemaximumof the dot products of the query token to all tokens in docu- nents.The query-document similarity scoreisaggregated oysummationoverthesimilarityscoreofallquerytokens. A core problem in the task is how to create supervised latafortheneuralretriever.Asthereisnohumanrele- /ance judgment for the passages, we treat “\"helpful' passage as positive passage. We first identify questions that GPT- 3.5-Turboanswersincorrectlywhenprovidedwithoutany contextualevidence.Then,usingBM25,werecallnpas- sages, where n = 32, and concatenate each of them with the original question to serve as its context. Subsequently, GPT- 3.5-Turboisprompted toanswerthisquestion.Passagesre- sulting in correct answers are treated as positive samples, whereasthoseleadingtoincorrectanswersarecategorized ishardnegatives.Additionally,asubsetofpassagesisran- domly chosen to act as easy negative samples. In the full pipeline of our evidence retrieval stage, we itilize afusion of sparseretrieval anddenseretrieval as the first-phase recall model. Specifically, we normalize the coresassignedtoeachdocumentbyboththesparsere- riever and dense retriever, sumthem together,and then se- lect the top k entries. In this paper, k is consistently set to 32. Subsequently,weemploy across-encoder-basedre-rankerto reorder the recalled passages. LLM ReaderWithinthe LLM reader,evidence retrieved from medical extbooksprovidesthefoundationalcontextfortheinput question.This evidenceisconcatenatedwiththerespective question, subsequently serving as input for the LLM. To en- sure the model's understanding and its alignment with our objectives,thefollowingstructuredpromptisemployed: *You are a medical doctor Referring to the evidence pro- vided, please examine the subsequent medical query and ex- cutethesubsequenttasks: I. Critically evaluate the question alongside each potential answer subseguentlydeterminingthecorrectresponse. 2. Appraise the likelihood of correctness for every option, ratingitonascalefromOto10.\" Upon employing this structured prompt, the LLM pro- duces answers utilizing the Chain-of-Thought prompting method.Eachretrievedevidencewillbeusedtogenerate adistinctanswerforthesamequestionincludingtheconfi- dence score p of each option k and the selected option ans oased on i-th evidence. We then combine N answers using a method based on majority voting, as detailed below: WithintheLLMreader,evidenceretrievedfrommedical extbooksprovidesthefoundationalcontextfortheinput question.This evidenceisconcatenatedwiththerespective question, subsequently serving as input for the LLM. To en- sure the model's understanding and its alignment with our objectives,thefollowingstructuredpromptisemployed: *You are a medical doctor Referring to the evidence pro- ided, please examine the subsequent medical query and ex- cutethesubsequenttasks: I. Critically evaluate the question alongside each potential answer subsequentlydeterminingthecorrectresponse. 2. Appraise the likelihood of correctness for every option, ratingitonascalefromOto10.\" Upon employing this structured prompt, the LLM pro- duces answers utilizing the Chain-of-Thought prompting nethod.Eachretrieved evidencewillbeusedtogenerate distinctanswerforthesamequestionincludingtheconfi- dence score p of each option k and the selected option ans oased on i-th evidence. We then combine N answers using a method based on majority voting, as detailed below: Ans = argmaxk 2 C I[ansi == k] * pi wheretheconfidencescoresforeachoptionaresummedup and the option with the highest confidence score is selected. Inthisformula,Irepresentsanindicatorfunction,which outputs a value of 1 if the condition within its brackets is satisfied (true) and O otherwise. This ensures that the confi- dencescoreisonlyconsideredinthesummationifthecor- responding answer matches the current option k. 4ExperimentsDatasetsWe evaluate LLM-AMT on three public medical open- domain multiple-choice QA datasets as follows: MedOA-USMLE and MedOA-MCMLE (Jin et al. 2021) originate from professional medical board exams in the USA and Mainland China, where doctors are evaluated ontheirprofessionalknowledgeandabilitytomakeclinical decisions. Questions in these exams are varied and generally requireadeepunderstandingofrelatedmedicalconcepts frommedicaltextbookstoanswer.Inadditiontotheques- tionsandcorrespondinganswers,thedatasetsalsoprovide associated medical textbook materials. For the USMLE, the MedQA-USMLE dataset includes text extracted from a to- tal of 18 English medical textbooks used by USMLE candi- dates.FortheMCMLE,theMedQA-MCMLEdatasetfea- uresmaterialsfrom33simplifiedChinesemedicaltext- oooks.Thesearedesignatedastheofficialtextbooksfor preparing for the medical licensing exam in Mainland China. MedMCQA  (Pal, Umapathi, and Sankarasubbu 2022) en- compasses a broad spectrum of 2,400 healthcare topics and 21distinctmedical subjects.Thediversityofquestionscon- tainedwithinMedMCQAillustratesthechallengesthatare uniquetothisdataset.Asthequestionsarederivedfrom oothreal-worldscenariosandsimulated examinations,they are meticulously crafted by human experts in the field. Con- sequently, these questions could serve as a comprehensive evaluation of a medical practitioner's professional compe- tencies and expertise. able NumberofQuestionsinMedQA-USMl MedQA-MCMLE, and MedMCQA Table2showsthedetailoftrain/dev/testsplitsofthe datasets. We evaluate our pipeline and conduct ablation stud- ies on the test sets of each dataset. BaselinesOurevaluationsencompasstwoprimarycategoriesofmod- els. The first group consists of the Closed-Book Models. which are pre-trained or fine-tuned specifically for the med- ical domain. These models rely on their internal knowl- edge and do not access external databases or texts during thequestion-answeringprocess. Notable models in this cat- egory include BioBERT, SciBERT, BioLinkBERT, and PubmedBERT (Lee et al. 2020; Beltagy, Lo, and Cohan 2019; Yasunaga, Leskovec, and Liang 2022; Gu et al. 2021). The second group is Wikipedia-Augmented Models. These models leverage the knowledge embedded in the Wikipedia corpus to assist in the medical question-answering task. Key models in this category are Variational ODQA (Liévin et al.) 2023), Codex 5-shot CoT (Liévin, Hother, and Winther 2022), and our replicated models using GPT-3.5-Turbo and LLaMA-2-13B (Touvron et al. 2023) that retrieve evidence from Wikipedia to augment the LLM reader. Implementation DetailsWe employed OpenAI's GPT-3.5-Turbo and LLaMA-2-13B as our LLM readers in different experiments. GPT-3.5- Turbo, accessed via its APl', handled query rewriting dur- ing the augmentation phase. In the evidence retrieval stage, SPLADE acted as our sparse retriever, DPR was the dense etriever,andweincorporatedacross-encoderforreranking. TheMS-MARCOdatasetwasourprimarytrainingsource for our zero-shot model. Specifics related to fine-tuning, such as batch size, learning rate, and training rounds, can befoundinthesupplementarymaterial Main ResultInTable3,wecomparevariousstate-of-the-artmodelswith our proposed pipeline on MedQA and MedMCQA datasets. When compared with closed-book models, our approach, supplemented with textbook knowledge, enhances the GPT- 3.5-Turbo'sbaselineperformancebymarginsof11.4%and 3.2%.Otherdomain-specificmodels,likeBioBERT,SciB. ERT, and PubmedBERT, posted accuracy scores of 36.7%, 39.0%,and 50.3% respectively on the MedQA dataset. While these scores underline the efficacy of domain-centric models, our textbook-augmented method still manages to surpass them. We can also observe a similar trend on the MedMCOAdataset Comparing our model to the GPT-3.5 + Wiki and LLaMA + Wiki setup, while Wikipedia is a rich source, its knowledge maybemoregeneralizedandcanlackthedepthrequired for specialized fields like medicine. Often, it tends towards orovidingmorelaymanorpopularizedinsights,makingit potentially less valuable for rigorous academic or clinical inquiries. Furthermore, GPT-3.5-turbo might already con- tain Wikipedia information from its pre-training, possibly eading to minimal performance gains (1.0% increase for MedQA and 1.7% increase for MedMCQA). Incontrast,ourmethodleveragesdetailedinsightsfrom nedical textbooks, giving us a clear advantage. We achieve 12.2% increase over GPT-3.5 + Wiki for MedQA and 9.7% orMedMCQA.Theperformancedistinctionemphasizes the significance of integrating deep, specialized medical knowledge over broad, surface-level information sources. Component Impact Analysis In our research, we metic- ulously explored the individual impacts of various compo- nents in our designed pipeline, as presented in Table 4. We https:/platform.openai.com/docs/guides/gpt Retriever MedQA- Med USMLE MC Method Wikipedia-Augmented Model Table 3:Performance of various state-of-the-art modelson MedQAandMedMCQAdatasets able3:Performanceofvariousstate-of-the-artmodels MedQAandMedMCQAdatasets demonstratetheinsightsderivedusingtheexampleofresults rom the MedQA-USMLE dataset.The trends we observed are similar in the other two datasets as well. Textbook Retriever:Introducing a retrieval mechanism significantly enhanced the performance. This is evident when we observe the leap in accuracy from the baseline GPT-3.5-Turbo score of 51.3% to 58.6% on the MedQA- USMLE dataset upon adding the retriever. It emphasizes the utility of integrating external knowledge bases to ex- tractcontextuallypertinentinformation.Moreover,fine tuning the retriever further bolstered the performance, pushing the accuracy from 62.0% to 64.1% for the “+ retriever + augmented query\" configuration. QueryAugmenter:Theefficacy of thequeryaugmenter is highlighted when contrastingtheresults of+re- triever\" with “\"+ retriever + augmented query\". Specif- ically, for the MedQA-USMLE dataset, accuracy in- creased from 58.6% to 62.0% after integrating the aug- mented query. This underscores the significance of query augmentation in furnishing the retriever with a deeper context and diverse relevant terminologies,thereby en- hancing its capability to procure more pertinent evidence. .LLM Reader: The inclusion of Majority Voting in ourarchitecturefurther boost the performance.Forthe MedQA-USMLE dataset, for example, the score grew from 62.0% (with augmented queries) to 62.3% af- ter incorporating majority voting. This shows the LLM reader's abilityto consolidate diverseinsightsfromdif- ferent sources through Majority Voting, which creates al Ablation Studywe perform ablation studies on both the retrieval lanisms and the majority voting strategy to refine and lable4 approaches on three medical QA datasets. The table show- casestheincrementalimprovementsgainedbyintegrating different components. Specifically, the retriever employed is HybTextR, and the LLM Reader is GPT-3.5-Turbo. identify the most optimal configuration specifically tailored for question-answering tasks within the medical domain Textbook In Table 5, evaluate the per- formanceimpact of various retrieval mechanisms within our pipeline. The late-interaction retriever, particularly the ColBERTmethod,demonstrates superior accuracy with 58.2%onMedQA-USMLE.62.4%onMedQA-MCMLE. and 58.1% on MedMCQA in the zero-shot setting, outper- formingboththestandalonesparseanddenseretrievers. When combining the semantic capabilities of the dense retriever with the precision of the sparse retriever, the system achievesbetterresults.The\"Sparse+Dense*combination vieldsanaccuracyof60.1%onMedQA-USMLE,64.9%on MedQA-MCMLE, and 58.7% on MedMCQA. The addition of thererankerfurther optimizesperfor- nance. The “Dense + Rerank\" configuration registers scores of 60.6%, 65.4%, and 61.8% across the three datasets, espectively.Notably,the“HybTextR\"whichintegrates sparse, dense, and reranker components, attains the high- est performances of 62.0% for MedQA-USMLE, 68.9% for MedQA-MCMLE, and 65.2% for MedMCQA.This consol- idated approach demonstrates the benefits of a comprehen- sive retrieval strategy in the medical domain. MajorityVotingInourLLMreadermodulerelatedto majorityvoting,we conducted experiments on different datasetswithvarioustop-ksettings,distinguishingbetween cases with and without fine-tuning,as shown in Figure 2. Theresults indicate thatfor the fine-tunedretriever, the op- imal setting istop-k=2,whilefor theretriever without fine- uning,thebestperformanceisachievedwithtop-k=4.This suggeststhatfine-tuningsignificantlyalterstheoptimaltop- kconfigurationformaximizingaccuracy Eine-tunedl Vero-shot MedQA-USMLE MedQA-MCMLE MedMCQA MedQA-USMLE MedQA-MCMLE MedMCQA Figure 2: Accuracy of LLM-AMT Under Different Top-k Selections in Majority Voting. GPT-3.5-Turbo was used as the LLM Reader. Figure 2: Accuracy of LLM-AMT Under Different Top-k Selections in Majority Voting. GPT-3.5-Turbo was used as the LLM Reader. Further DiscussionInthissection,weprovideanin-depthdiscussionandfur- ther assessment of our models, especially their capabilities in handling non-multiple-choice medical QA tasks. application of models in medical question answering,we randomly selected adiverse subset of100questions from the MedQA-USMLE dataset and generated answers directly without accessingtheoptions.Forevaluation,individuals withamedicalbackgroundwereenlistedtocriticallyassess thequalityofeachanswer.Theseanswerswerecategorized into four distinct tiers: Table6:Evaluationof the Non-multiple-choice Medical Question Answering Task. GPT-3.5 as the LLM Reader. ole6:EvaluationoftheNon-multiple-choiceMedica estion Answering Task. GPT-3.5 as the LLM Reader. ullyaccurateandcomprehensive MostlyCorrect:Accuratewithminoromissions • Partially Correct: Contains accurate elements but misses vital details in the response. . Wrong: Largely inaccurate or off-target. Our LLM-AMT model surpassed the GPT-3.5-Turbo oaseline in the non-multiple-choice QA task, delivering 36 correct answers to the baseline's 27. Notably, LLM-AMT provided more partially correct answers (19 vs.14) and fewer errors (33 vs. 49). This underscores the model's en- nancedaccuracyinthemedicalQAdomain,asdetailedin Table6.The superiorperformanceof LLM-AMTinthenon- multiple-choice QA task not only illustrates its advanced capabilitiesbutalsoemphasizesitspotentialforpractical application in real-world medical scenarios. Such advance- nents can be instrumental in aiding medical professionals with more accurate and reliable information. 5 ConclusionWeintroducedLLM-AMT,anovelpipelineoptimizedfor nedicaltasks.harnessingauthoritativemedicaltextbooks oenhanceLLMs'accuracy andprofessionalism.Empiri- calevaluationsreinforcedthevalueofintegratingdomain pecifictextbookswithLLMs,providinganavenueforfu ure studies. Further, our ablation study delineated the sig. nificanceof externalknowledgeretrieval,queryaugmenta- ion,andmajorityvotingmechanismswithinourproposed rchitecture.Thesefindingssetaprecedentforadvancing specialized domain-aware models, especially in the context of medical informatics and healthcare AI applications. ReferencesBeltagy, I.; Lo, K.; and Cohan, A. 2019. SciBERT: A pre- trained language model for scientific text. arXiv preprint arXiv:1903.10676 Borgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther- ford, E.; Millican, K.; Van Den Driessche, G. B.; Lespiau. J.-B.; Damoc, B.; Clark, A.; et al. 2022. Improving language models by retrieving from trillions of tokens. In Interna- tional conference on machine learning, 2206-2240. PMLR. Chen, D.; Fisch, A.; Weston, J.; and Bordes, A. 2017. Read- ing wikipedia to answer open-domain questions. arXiv preprintarXiv:1704.00051. Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.: Gehrmann, S.; et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fe- lus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al. 2022. Scaling instruction-finetuned language models. arXiv preprintarXiv:2210.11416. Ely, J. W.; Osheroff, J. A.; Chambliss, M. L.; Ebell, M. H.: and Rosenbaum,M.E.2005.Answering physicians' clinical questions: obstacles and potential solutions.Journal of the American Medical Informatics Association, 12(2): 217-224 Formal, T; Piwowarski, B.; and Clinchant, S. 2021. SPLADE: Sparse lexical and expansion model for first stage ranking. In Proceedings of the 44th International ACM SI GIR Conference on Research and Development in Informa- tionRetrieval.2288-2292. Gao, J.; Xiong, C.; Bennett, P.; and Craswell, N. 2022. Neu- ral approaches to conversational information retrieval. arXiv preprint arXiv:2201.05176. Gao, L.; and Callan, J. 2021. Condenser: a pre- training architecture for dense retrieval. arXiv preprint arXiv:2104.08253. Gao, L.; Dai, Z.; and Callan, J. 2021. COIL: Revisit exact exicalmatchininformationretrievalwithcontextualizedin- verted list. arXiv preprint arXiv:2104.07186. Gu, Y.: Tinn.R.: Cheng,H.:Lucas, M.: Usuyama.N.:Liu. X.; Naumann, T.; Gao, J.; and Poon, H. 2021. Domain- specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1): 1-23. Guu, K.; Lee, K.; Tung, Z.; Pasupat, P; and Chang, M. 2020. Retrieval augmented language model pre-training.In International conference on machine learning, 3929-3938. PMLR. Izacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski, P; Joulin, A.; and Grave, E. 2021. Unsupervised dense in- formation retrieval with contrastive learning. arXiv preprint arXiv:2112.09118. zacard, G.;Lewis,P.:Lomeli,M.;Hosseini,L.;Petroni,F.: Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave, E. 2022. Few-shot learning with retrieval augmented lan- guage models. arXiv preprint arXiv:2208.03299 Jin, D.; Pan, E.; Oufattole, N.; Weng,W.-H.; Fang, H.; and Szolovits, P. 2021. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14): 6421. Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W. W.; and Lu, X. 2019. Pubmedga: A dataset for biomedical research question an- swering. arXiv preprint arXiv:1909.06146. Jin, Q.; Yuan, Z.; Xiong, G.; Yu, Q.; Ying, H.; Tan, C.; Chen, M.; Huang, S.; Liu, X.; and Yu, S. 2022. Biomedical ques- ion answering: a survey of approaches and challenges. ACM Computing Surveys (CSUR), 55(2): 1-36. Karpukhin, V; Oguz, B.; Min, S.; Lewis, P; Wu, L.; Edunov, S.; Chen, D.; and Yih, W.-t. 2020. Dense passage re- rieval for open-domain question answering. arXiv preprint arXiv:2004.04906. Khattab, O.; and Zaharia, M. 2020. Colbert: Efficient and effective passage search via contextualized late interaction overbert.In Proceedings of the 43rd International ACM SI- GiRconferenceonresearchanddevelopmentinInformation Retrieval,39-48. Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and Kang,J.2020.BioBERT:apre-trainedbiomedicallanguage representation model for biomedical text mining. Bioinfor- matics, 36(4): 1234-1240. Lewis, P.: Perez, E.; Piktus, A.; Petroni, F; Karpukhin, V.: Goyal, N.; Kittler, H.; Lewis, M.; Yih, W.-t.; Rocktaschel, T.; et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neural Infor- mation Processing Systems, 33: 9459-9474. Liévin, V.; Hother, C. E.; and Winther, O. 2022. Can large language models reason about medical questions? arXiy preprint arXiv:2207.08143. Liévin, V.; Motzfeldt, A. G.; Jensen, I. R.; and Winther O. 2023. Variational Open-Domain Question Answering In International Conference on Machine Learning, 20950 20977.PMLR. Lin, J.; and Ma, X. 2021. A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques. arXiv preprint arXiv:2106.14807. Luo, R.; Sun, L.; Xia, Y.; Qin, T.; Zhang, S.; Poon, H.; and Liu, T.-Y. 2022. BioGPT: generative pre-trained trans former for biomedical text generation and mining. Briefings in Bioinformatics,23(6):bbac409. Mallia, A.; Khattab, O.; Suel, T.; and Tonellotto, N. 2021 Learningpassageimpactsforinvertedindexes.InProceed- ings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1723- 1727. Mialon, G.; Dessi, R.; Lomeli, M.; Nalmpantis, C.; Pa sunuru, R.; Raileanu, R.; Roziere, B.; Schick, T.; Dwivedi- Yu,J.:Celikyilmaz,A.:et al.2023.Augmentedlanguage models: a survey. arXiv preprint arXiv:2302.07842. Pal, A.; Umapathi, L. K.; and Sankarasubbu, M. 2022. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health. Inference. and Learnine 248-260. PMLR Peng,B.;Galley,M.;He,P.;Cheng,H.:Xie,Y.;Hu,Y.; Huang, Q.; Liden, L.; Yu, Z.; Chen, W.; et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprintarXiv:2302.12813. Ram, O.; Levine, Y.; Dalmedigos, I.; Muhlgay, D.; Shashua, A.; Leyton-Brown, K.; and Shoham, Y. 2023.In-context retrieval-augmented language models.arXiv preprint arXiv:2302.00083. Shi,W.;Min,S.;Yasunaga,M.:Seo,M.;James,R.;Lewis. M.; Zettlemoyer, L.; and Yih, W.-t. 2023. Replug: Retrieval- augmented black-box language models. arXiv preprint arXiv:2301.12652. Singhal, K.; Tu, T.; Gottweis, J.; Sayres, R.; Wulczyn, E.; Hou, L.; Clark, K.; Pfohl, S.; Cole-Lewis, H.; Neal, D.; et al. 2023. Towards expert-level medical question answering with large language models.arXiv preprint arXiv:2305.09617. Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei,Y.:Bashlykov,N.;Batra,S.Bhargava,P.;Bhosale. S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler, D.; et al. 2022a. Emergent abilities of large language mod- els.arXiv preprint arXiv:2206.07682. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.: Chi.E.;Le.Q.V.:Zhou.D.:et al.2022b.Chain-of- hought prompting elicits reasoning in large language mod- els. Advances in Neural Information Processing Systems, 35:24824-24837. Wu, B.; Zhang, Z.; Wang, J.; and Zhao, H. 2021. Sentence- aware contrastive learning for open-domain passage re- trieval.arXiv preprint arXiv:2110.07524. Xiong, L.; Xiong, C.; Li, Y.; Tang, K.-F.; Liu, J.; Bennett, P.; Ahmed, J.; and Overwijk, A. 2020. Approximate near- est neighbor negative contrastive learning for dense text re- trieval. arXiv preprint arXiv:2007.00808. Yasunaga, M.; Leskovec, J.; and Liang, P. 2022. Linkbert: Pretraining language models with document links. arXiv preprint arXiv:2203.15827. Zhang, X.; Wu, J.; He, Z.; Liu, X.; and Su, Y. 2018. Medical examquestion answering withlarge-scalereading compre- hension. In Proceedings of the AAAI conference on artificial intelligence.volume 32. ", "need_gpt": true, "gpt_correct_result": ""}